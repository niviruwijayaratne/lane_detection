{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerUtil:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def conv(self, in_channels, filters, kernel_size):\n",
    "        return nn.Conv2d(in_channels = in_channels, out_channels = filters, kernel_size = kernel_size)\n",
    "    \n",
    "    def pool(self, kernel_size = (2,2), stride = 2):\n",
    "        return nn.MaxPool2d(kernel_size = kernel_size, stride = stride)\n",
    "    \n",
    "    def deconv(self, in_channels, filters, kernel_size, stride = 2):\n",
    "            return nn.ConvTranspose2d(in_channels = in_channels, out_channels = filters, kernel_size = kernel_size, stride = 2)\n",
    "        \n",
    "    def crop_and_copy(self, encoder_tensor, decoder_tensor):\n",
    "        encoder_tensor_width = np.squeeze(np.asarray(encoder_tensor[0].shape[1]))\n",
    "        encoder_tensor_height = np.squeeze(np.asarray(encoder_tensor[0].shape[2]))\n",
    "        decoder_tensor_width = np.squeeze(np.asarray(decoder_tensor[0].shape[1]))\n",
    "        decoder_tensor_height = np.squeeze(np.asarray(decoder_tensor[0].shape[2]))\n",
    "        decoder_tensor_channels = np.squeeze(np.asarray(decoder_tensor[0].shape[0]))\n",
    "        crop_width = (encoder_tensor_width - decoder_tensor_width) // 2\n",
    "        crop_height = (encoder_tensor_height - decoder_tensor_height) // 2\n",
    "        reshaped_encoder = torch.rand(12, decoder_tensor_channels, decoder_tensor_width, decoder_tensor_height)\n",
    "        \n",
    "        for i in range(12):\n",
    "            reshaped_encoder[i] = encoder_tensor[i][:, crop_width:encoder_tensor_width - crop_width, \n",
    "                                                    crop_height:encoder_tensor_height - crop_height]\n",
    "        output = torch.cat((reshaped_encoder, decoder_tensor), 1)\n",
    "        return output\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.helper = LayerUtil()\n",
    "        self.conv1_1 = self.helper.conv(3, 64, (3,3))\n",
    "        self.conv1_2 = self.helper.conv(64, 64, (3,3))\n",
    "        self.pool1 = self.helper.pool((2,2), 2)\n",
    "        \n",
    "        self.conv2_1 = self.helper.conv(64, 128, (3,3))\n",
    "        self.conv2_2 = self.helper.conv(128, 128, (3,3))\n",
    "        self.pool2 = self.helper.pool((2,2), 2)\n",
    "        \n",
    "        self.conv3_1 = self.helper.conv(128, 256, (3,3))\n",
    "        self.conv3_2 = self.helper.conv(256, 256, (3,3))\n",
    "        self.pool3 = self.helper.pool((2,2), 2)\n",
    "        \n",
    "        self.conv4_1 = self.helper.conv(256, 512, (3,3))\n",
    "        self.conv4_2 = self.helper.conv(512, 512, (3,3))\n",
    "        self.pool4 = self.helper.pool((2,2), 2)\n",
    "        \n",
    "        self.conv5_1 = self.helper.conv(512, 1024, (3,3))\n",
    "        self.conv5_2 = self.helper.conv(1024, 1024, (3,3))\n",
    "        self.deconv1 = self.helper.deconv(1024, 512, (2,2))\n",
    "        \n",
    "        self.conv6_1 = self.helper.conv(1024, 512, (3,3))\n",
    "        self.conv6_2 = self.helper.conv(512, 512, (3,3))\n",
    "        self.deconv2 = self.helper.deconv(512, 256, (2,2))\n",
    "        \n",
    "        self.conv7_1 = self.helper.conv(512, 256, (3,3))\n",
    "        self.conv7_2 = self.helper.conv(256, 256, (3,3))\n",
    "        self.deconv3 = self.helper.deconv(256, 128, (2,2))\n",
    "        \n",
    "        self.conv8_1 = self.helper.conv(256, 128, (3,3))\n",
    "        self.conv8_2 = self.helper.conv(128, 128, (3,3))\n",
    "        self.deconv4 = self.helper.deconv(128, 64, (2,2))\n",
    "        \n",
    "        self.conv9_1 = self.helper.conv(128, 64, (3,3))\n",
    "        self.conv9_2 = self.helper.conv(64, 64, (3,3))\n",
    "        self.conv9_3 = self.helper.conv(64, 2, (1,1))\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        result = self.conv1_1(inputs)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv1_2(result)\n",
    "        concat1 = F.relu(result)\n",
    "        result = self.pool1(concat1)\n",
    "        \n",
    "        result = self.conv2_1(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv2_2(result)\n",
    "        concat2 = F.relu(result)\n",
    "        result = self.pool2(concat2)\n",
    "        \n",
    "        result = self.conv3_1(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv3_2(result)\n",
    "        concat3 = F.relu(result)\n",
    "        result = self.pool3(result)\n",
    "        \n",
    "        result = self.conv4_1(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv4_2(result)\n",
    "        concat4 = F.relu(result)\n",
    "        result = self.pool4(result)\n",
    "        \n",
    "        result = self.conv5_1(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv5_2(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.deconv1(result)\n",
    "        result = self.helper.crop_and_copy(concat4, result)\n",
    "        \n",
    "        result = self.conv6_1(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv6_2(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.deconv2(result)\n",
    "        result = self.helper.crop_and_copy(concat3, result)\n",
    "        \n",
    "        result = self.conv7_1(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv7_2(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.deconv3(result)\n",
    "        result = self.helper.crop_and_copy(concat2, result)\n",
    "        \n",
    "        result = self.conv8_1(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv8_2(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.deconv4(result)\n",
    "        result = self.helper.crop_and_copy(concat1, result)\n",
    "        \n",
    "        result = self.conv9_1(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv9_2(result)\n",
    "        result = F.relu(result)\n",
    "        result = self.conv9_3(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0113, 0.0142, 0.0105,  ..., 0.0141, 0.0082, 0.0123],\n",
       "         [0.0077, 0.0055, 0.0057,  ..., 0.0079, 0.0083, 0.0058],\n",
       "         [0.0093, 0.0129, 0.0133,  ..., 0.0086, 0.0101, 0.0158],\n",
       "         ...,\n",
       "         [0.0081, 0.0097, 0.0107,  ..., 0.0083, 0.0124, 0.0137],\n",
       "         [0.0144, 0.0104, 0.0116,  ..., 0.0134, 0.0101, 0.0051],\n",
       "         [0.0102, 0.0127, 0.0098,  ..., 0.0104, 0.0078, 0.0149]],\n",
       "\n",
       "        [[0.0841, 0.0870, 0.0868,  ..., 0.0903, 0.0871, 0.0823],\n",
       "         [0.0892, 0.0848, 0.0830,  ..., 0.0860, 0.0875, 0.0867],\n",
       "         [0.0865, 0.0887, 0.0831,  ..., 0.0858, 0.0859, 0.0893],\n",
       "         ...,\n",
       "         [0.0820, 0.0866, 0.0869,  ..., 0.0903, 0.0854, 0.0838],\n",
       "         [0.0859, 0.0872, 0.0843,  ..., 0.0906, 0.0866, 0.0840],\n",
       "         [0.0851, 0.0844, 0.0856,  ..., 0.0827, 0.0868, 0.0832]]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_tensor = torch.rand(12, 3, 572,572)\n",
    "input_tensor.to(device)\n",
    "final = model(input_tensor)\n",
    "final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(final[0][0].detach().numpy())\n",
    "# plt.imshow(final[0][1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/niviru/Desktop/Research/VOCtrainval_11-May-2012.tar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.voc.VOCSegmentation"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "install_path = os.path.join(\"/Users\", \"niviru\", \"Desktop\", \"Research\")\n",
    "train_data = torchvision.datasets.VOCSegmentation(root = install_path, download = True)\n",
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAEZCAMAAABW0ifkAAADAFBMVEUAAACAAAAAgACAgAAAAICAAIAAgICAgIBAAADAAABAgADAgABAAIDAAIBAgIDAgIAAQACAQAAAwACAwAAAQICAQIAAwICAwIBAQADAQABAwADAwABAQIDAQIBAwIDAwIAAAECAAEAAgECAgEAAAMCAAMAAgMCAgMBAAEDAAEBAgEDAgEBAAMDAAMBAgMDAgMAAQECAQEAAwECAwEAAQMCAQMAAwMCAwMBAQEDAQEBAwEDAwEBAQMDAQMBAwMDAwMAgAACgAAAggACggAAgAICgAIAggICggIBgAADgAABggADggABgAIDgAIBggIDggIAgQACgQAAgwACgwAAgQICgQIAgwICgwIBgQADgQABgwADgwABgQIDgQIBgwIDgwIAgAECgAEAggECggEAgAMCgAMAggMCggMBgAEDgAEBggEDggEBgAMDgAMBggMDggMAgQECgQEAgwECgwEAgQMCgQMAgwMCgwMBgQEDgQEBgwEDgwEBgQMDgQMBgwMDgwMAAIACAIAAAoACAoAAAIICAIIAAoICAoIBAIADAIABAoADAoABAIIDAIIBAoIDAoIAAYACAYAAA4ACA4AAAYICAYIAA4ICA4IBAYADAYABA4ADA4ABAYIDAYIBA4IDA4IAAIECAIEAAoECAoEAAIMCAIMAAoMCAoMBAIEDAIEBAoEDAoEBAIMDAIMBAoMDAoMAAYECAYEAA4ECA4EAAYMCAYMAA4MCA4MBAYEDAYEBA4EDA4EBAYMDAYMBA4MDA4MAgIACgIAAgoACgoAAgIICgIIAgoICgoIBgIADgIABgoADgoABgIIDgIIBgoIDgoIAgYACgYAAg4ACg4AAgYICgYIAg4ICg4IBgYADgYABg4ADg4ABgYIDgYIBg4IDg4IAgIECgIEAgoECgoEAgIMCgIMAgoMCgoMBgIEDgIEBgoEDgoEBgIMDgIMBgoMDgoMAgYECgYEAg4ECg4EAgYMCgYMAg4MCg4MBgYEDgYEBg4EDg4EBgYMDgYMBg4MDg4MCa7rFGAAAFw0lEQVR4nO3di47jKBBA0c4X5P+/tkaTJ2CIeRko6h5ppZ1WO8FcY8fu2d6/PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANghD7NHgZFEqG6NCNWtEaG6Nc/atxvVDXk1f1WfPRqM8Gn+rD57OBhAvtFZ6UaEzYm+PwlO7jTfn9ucK7oRbnOiG+E2J7oRRLfl8+DVbU70HckR0XcViR005+yuXbpxsjnR+xk/kRW9id7NYZanvOlJ6uLog/dHj9OZn/DOydhl0Sccx+s77X3pZBU0jjU/G9boQ1iDzODXzVNt8Fvej9iGH8MaFES/ZJrqk6eau1/9DN3bxHr1kubXRa9PfhhTfOTH7R7bjtjBBc2OXtk8OaST4pHy9qoXNV8menpEmc1NZy9rflX0yuDp5u435b2Qperz9/t3mN+Vml7tdjwmrFSf7mw1JpMnXy2z+S3yzlQfxJt775TsV8k5F5Us9P8H1PF7iT6E0/nb1Ml8bJ7zUvmC7yf6EIdLtR89/6+3exvnBX/9Q/ThUtHzLuPBy5Qt88hxQPQx/LCRs/245kQfx+36rjelOdEn8UpPaE70GXJXd7hFp+ZEn6K0ef2P6mLRL9wx/DCsuRwfxF64W7aEH8rO1Lx69cq+Pnrj/ulU2rxwgmqbS3i09ArS+6jWqKl5xjy1Nfef/zX06LQ3e+jTPD1vjz80NG+s0Tj6TRVHKZs4qYue3aIyqu/41oMjjFbVpPAAqH3dsKk36F6Ro28+McgIdVXikTo1b0zaOAyxEr1H+HS3tq0HtD5uNLvK1a6pnvW3HwsqNEUtfvnZTa5XMsOdj4ysNNFGDa93bnaREYqqzzkAWhTv1+wew+zbnc5pldX7HwnXfbYgcpWa/ledDSqOxtnTp19m9eKFf/b9xJ4rs3v5Cj78oaI0zQdqjX4Lf5zaGn32hBjTHP29PTUVqYzu/wz9EH32XuG3iuixywOpNekYnepa9IxOdSUqr+nx6FTXoXalv7d9PdwRTvCKtEb3vzx7b5ClMrq7Lf//Tm3KowvRtSuOLkRXrzS6+yn988n9xvMZVQqje3dmzu0a0TUpi+7fjRNdKSmq7t+ME12p8ujhtkJ0dUqiSyT6+1EsT+QUyV/qImFYCU3bCxQpjX7cmOb65FaPdn1+jebaSFb1311prk1Oddbybs6r03w/n/sumhvyKzofzXeVqM7t2NbOqs8eHy4RrU7zzcWqU3x3RLcoEZ3qO0te1GcPDJdJf36fPTJcJv6AhuhbSzYn+m6+V+34k1ii78d55sbp3QgJcXrf3qE5N+r7eyS935PVab6hZ/O7l52T++Y+0e/Pf3Gr85+tbMqN/l3x70XOQt+SOOf3e3ie/9zLIZOOCftGv6eizx6iIlomzVnq8eizB6iImml7jfFzfqd5NT0T50ZXMeB1vSZy/Un8Nr9rGO7KPhMp98Wn8TnSV/TVB7s2Z/Xc155HZ6DLj3Vt4k3l0hPpR197rEsTopsTab7uRBK9i6A50S2QyDyuO5NE7yKYxsWXOtG7OERfujrRuyC6QcfoK08l0bsgukHhp/e1z+9E7yK8Tye6BaqWOtE70RZdiN4u8VFuydkMLkarDnN96qKvP8z1RaIvu4SOZ/clh7k+3dFnj0gpohtEdIOIbpCmpzNE7+T4IFZ0RF92lBrEo88eVRTRuwmrrzuZ7kBp3kTEO8UvPJuvUcrS1yAlvOgrz2YYffZ4VHOir9w8jD57OMoF0WcPJ4XoXXnRZw8miehdEd0gohtEdIOIbhDRDdIVXda+s9RCT/T3r8BaeZxK6Hk4IzTvRUd0Pb/PVAXRcX5X85uLVdAS/Y/gHWmJjo6IbhDRDXpf1LlmGiLug4/Zg8Eg3AFbRHOLaG4RzQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAmOofvU0cjXDMMl4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=P size=500x281 at 0x12E8BA750>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 5\n",
    "train_losses = np.zeros(5)\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in train_data:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_losses[epoch] = loss\n",
    "    print(\"Epoch {}/{}\".format(epoch, epochs) + \": Loss is {}\".format(train_losses[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
